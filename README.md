<img width="1289" height="862" alt="image" src="https://github.com/user-attachments/assets/abf629bf-2896-4ca3-9396-2ff486e9bdc7" />

# Analytics Engineering E2E Platform (Event Metrics + Data Trust)

Production-ready analytics engineering platform that turns raw, event-level data into trusted, BI-ready metrics.

This repo demonstrates an end-to-end pattern used in modern, high-signal data teams:
- **Object storage ingest** (S3 compatible via MinIO for local dev)
- **dbt transformations** (staging → ods → marts) for clean, governed data products
- **Data trust gates** using **dbt tests** and **Great Expectations** (freshness, validity, and anomaly checks)
- **Airflow orchestration** for repeatable daily runs
- **BI exports** to support QuickSight, Tableau, or Power BI

The project is intentionally designed around **event-style pipelines**, where correctness, freshness, and reliability matter as much as speed.


## Stack
- Python 3.11
- Airflow 2.8.4
- dbt-core/dbt-postgres 1.7.x (Redshift-like local target)
- Great Expectations 0.18.x
- MinIO for S3 emulation
- Postgres for Airflow metadata + warehouse

## Quickstart

```bash
cp .env.example .env
make init
make up
make airflow-init
```

Once Airflow is running:
- UI: http://localhost:8080 (admin/admin)
- MinIO console: http://localhost:9001

Trigger the DAG:
```bash
make airflow-trigger
```

Or run locally (outside Airflow):
```bash
source .venv/bin/activate
python src/ingest/ingest_to_s3.py
DBT_PROFILES_DIR=./dbt dbt run --project-dir dbt
DBT_PROFILES_DIR=./dbt dbt test --project-dir dbt
great_expectations checkpoint run analytics_checkpoint --config great_expectations/great_expectations.yml
python src/ingest/export_daily_metrics.py
```

## Data Source
- Publicly available NYC TLC Yellow Taxi trip records.
- Source URL: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet
- The ingest demo uses a small subset of columns from this single monthly parquet file.

## Project Structure
```
/infra
  docker-compose.yml
  airflow/
    dags/
    Dockerfile
/src
  ingest/
    ingest_to_s3.py
    export_daily_metrics.py
/dbt
  dbt_project.yml
  profiles.yml.example
  models/
    staging/
    ods/
    marts/
/great_expectations
  great_expectations.yml
  expectations/
  checkpoints/
/dashboards
  README.md
  screenshots/
/docs
  architecture.md
  data_dictionary.md
  lineage.md
```

## Switching from MinIO to AWS S3
1. Set `S3_ENDPOINT_URL` to empty or remove it.
2. Populate `AWS_ACCESS_KEY_ID`/`AWS_SECRET_ACCESS_KEY` with real AWS credentials.
3. Set `S3_BUCKET` to an existing S3 bucket.

## Switching Warehouse Targets
Default: **Postgres** (local Redshift-like).

### Redshift
- Update `dbt/profiles.yml` to use `type: redshift` and configure `host`, `user`, `password`.
- Update `WAREHOUSE_DB_URI` to a Redshift SQLAlchemy connection string.

### Databricks SQL
- Install `dbt-databricks` and update profiles.
- Update Airflow image requirements if needed.

## dbt Layers
- **staging**: raw cleanup (types, field names)
- **ods**: lightly transformed operational data
- **marts**: analytics-ready facts/dimensions

## Great Expectations
- `raw_yellow_taxi_suite` checks row counts, nulls, value ranges.
- `mart_daily_metrics_suite` checks mart-level KPI validity.
- Checkpoint: `analytics_checkpoint`.

## BI Exports
- `exports/daily_metrics.csv` generated by `src/ingest/export_daily_metrics.py`.
- Load into QuickSight/Tableau/Power BI.

## Makefile
- `make up` / `make down` / `make logs`
- `make dbt-run`, `make dbt-test`
- `make ge-check`
- `make airflow-init`, `make airflow-trigger`
- `make lint`, `make test`

## Documentation
- Architecture: `docs/architecture.md`
- Data dictionary: `docs/data_dictionary.md`
- Lineage: `docs/lineage.md`

## Manual Checklist
- [ ] Replace `AIRFLOW_FERNET_KEY` in `.env` (use `python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"`).
- [ ] Add real AWS credentials if using S3.
- [ ] Add BI dashboard screenshots under `dashboards/screenshots/`.
- [ ] Update README with real dashboard images.
