# Analytics Engineering E2E Platform

Production-ready analytics engineering starter with:
- **S3 ingest** (MinIO for local dev, S3 compatible)
- **dbt models** (staging → ods → marts)
- **dbt tests + Great Expectations checks**
- **Airflow orchestration**
- **BI-ready exports** (QuickSight/Tableau/Power BI)

## Stack
- Python 3.11
- Airflow 2.8.4
- dbt-core/dbt-postgres 1.7.x (Redshift-like local target)
- Great Expectations 0.18.x
- MinIO for S3 emulation
- Postgres for Airflow metadata + warehouse

## Quickstart

```bash
cp .env.example .env
make init
make up
make airflow-init
```

Once Airflow is running:
- UI: http://localhost:8080 (admin/admin)
- MinIO console: http://localhost:9001

Trigger the DAG:
```bash
make airflow-trigger
```

Or run locally (outside Airflow):
```bash
source .venv/bin/activate
python src/ingest/ingest_to_s3.py
DBT_PROFILES_DIR=./dbt dbt run --project-dir dbt
DBT_PROFILES_DIR=./dbt dbt test --project-dir dbt
great_expectations checkpoint run analytics_checkpoint --config great_expectations/great_expectations.yml
python src/ingest/export_daily_metrics.py
```

## Dataset
- NYC TLC Yellow Taxi dataset (official parquet).
- The ingest script downloads the file and loads **one parquet row group** to keep the demo light.
- Taxi zone lookup is also downloaded for dimension modeling.

## Project Structure
```
/infra
  docker-compose.yml
  airflow/
    dags/
    Dockerfile
/src
  ingest/
    ingest_to_s3.py
    export_daily_metrics.py
/dbt
  dbt_project.yml
  profiles.yml.example
  models/
    staging/
    ods/
    marts/
/great_expectations
  great_expectations.yml
  expectations/
  checkpoints/
/dashboards
  README.md
  screenshots/
/docs
  architecture.md
  data_dictionary.md
  lineage.md
```

## Switching from MinIO to AWS S3
1. Set `S3_ENDPOINT_URL` to empty or remove it.
2. Populate `AWS_ACCESS_KEY_ID`/`AWS_SECRET_ACCESS_KEY` with real AWS credentials.
3. Set `S3_BUCKET` to an existing S3 bucket.

## Switching Warehouse Targets
Default: **Postgres** (local Redshift-like).

### Redshift
- Update `dbt/profiles.yml` to use `type: redshift` and configure `host`, `user`, `password`.
- Update `WAREHOUSE_DB_URI` to a Redshift SQLAlchemy connection string.

### Databricks SQL
- Install `dbt-databricks` and update profiles.
- Update Airflow image requirements if needed.

## dbt Layers
- **staging**: raw cleanup (types, field names)
- **ods**: lightly transformed operational data
- **marts**: analytics-ready facts/dimensions

## Great Expectations
- `raw_yellow_taxi_suite` checks row counts, nulls, value ranges.
- `mart_daily_metrics_suite` checks mart-level KPI validity.
- Checkpoint: `analytics_checkpoint`.

## BI Exports
- `exports/daily_metrics.csv` generated by `src/ingest/export_daily_metrics.py`.
- Load into QuickSight/Tableau/Power BI.

## Makefile
- `make up` / `make down` / `make logs`
- `make dbt-run`, `make dbt-test`
- `make ge-check`
- `make airflow-init`, `make airflow-trigger`
- `make lint`, `make test`

## Documentation
- Architecture: `docs/architecture.md`
- Data dictionary: `docs/data_dictionary.md`
- Lineage: `docs/lineage.md`

## Manual Checklist
- [ ] Replace `AIRFLOW_FERNET_KEY` in `.env` (use `python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"`).
- [ ] Add real AWS credentials if using S3.
- [ ] Add BI dashboard screenshots under `dashboards/screenshots/`.
- [ ] Update README with real dashboard images.
